{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): keras in /anaconda/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): h5py in /anaconda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-preprocessing>=1.0.5 in /anaconda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyyaml in /anaconda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.14 in /anaconda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.9.1 in /anaconda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-applications>=1.0.6 in /anaconda/lib/python3.5/site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.9.0 in /anaconda/lib/python3.5/site-packages (from keras)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "##### By: (The one and only) James Bartlett, Edited by Ashley Chien\n",
    "\n",
    "Keras is a neural network framework that wraps tensorflow (if you haven't heard of tensorflow it's another neural network framework) and makes it really simple to implement common neural networks. Its philosophy is to make simple things easy (but beware, trying to implement uncommon, custom neural networks can be pretty challenging in Keras, for the purposes of this course you will never have to that though so don't worry about it). If you are ever confused during this homework, Keras has really good documentation, so you can go to [Keras Docs](https://keras.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Keras has many datasets conveniently built in to the library. We can access them from the ``keras.datasets`` module. For this homework, we will be using their housing price dataset, their image classification dataset and their movie review sentiment dataset. To get a full list of their datasets, you can go to this link. [Keras Datasets](https://keras.io/datasets). To use their datasets, we import them and then call ``load_data()``, load_data returns two tuples, the first one is training data, and the second one is testing data. See the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose the proportion of training data you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set before:  (404, 13)\n",
      "Size of training set after:  (455, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training set before: \", x_train.shape)\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.10)\n",
    "print(\"Size of training set after: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import normalize\n",
    "x_train = normalize(x_train, axis=1)\n",
    "x_test = normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Every thing in Keras starts out with a model. From an initial model, we can add layers, train the model on data, evaluate the model on test sets, etc. We initialize a model with ``Sequential()``. `Sequential` refers to the fact that the model has a sequence of layers. Personally, I have very rarely used anything other than `Sequential`, so I think it's all you really need to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model, we can add layers to it with ``model.add``. Keras has a really good range of layers we can use. For example, if we want a basic fully connected layer we can use ``Dense``. I will now run through an example of using Keras to build and train a fully connected neural network for the purposes of regressing on housing prices for the dataset we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "model.add(Dense(16, input_shape=(13,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code adds a fully connected layer with 16 neurons. For the first layer of any model we always have to specify the input shape. In our case we will be training a fully connected network on the Boston Housing data, so each data point has 13 features. That's why we use an input_shape of (13,). The nice part about Keras is other than the input_shape for the first layer, we don't have to worry about shapes the rest of the time, Keras takes care of it. This can be really useful when you are doing complicated convolutions and things like that where working out the input shape to the next layer can be non-trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add an Activation function to our network after our first fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple as that. We just added a relu activation to the whole layer. To see a list of activation functions available in Keras go to [Keras Activations](https://keras.io/activations/). Now let's add the final layer in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a handy utility in Keras to print out what our model looks like so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see it shows us what layers we have, the output shapes of each layer, and how many parameters there are for each layer. All this information can be really useful when trying to debug a model, or even for sharing your model architecture with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Now for actually training the model. Before we train a model we have to compile it. ``model.compile`` is how you specify which optimizer to use and what loss function to use. Sometimes choosing the right optimizer can have a significant effect on model performance. For a list of optimizers look at [Keras Optimizers](https://keras.io/optimizers). Choosing the right optimizer is mostly just trying each one to see which works better; there is some general advice for when to use each one but it is basically just another hyperparameter. We also have to choose a loss function. Choosing the right loss function is really important because the loss function decides what the goal of the model is. Since we are doing regression, we want to choose mean squared error, to get our output to be as close as possible to the label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to actually train our model on the data. This is really easy in Keras, in fact it only takes one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 207.5379\n",
      "Epoch 2/100\n",
      "455/455 [==============================] - 0s 61us/step - loss: 70.7795\n",
      "Epoch 3/100\n",
      "455/455 [==============================] - 0s 74us/step - loss: 68.4759\n",
      "Epoch 4/100\n",
      "455/455 [==============================] - 0s 80us/step - loss: 68.3049\n",
      "Epoch 5/100\n",
      "455/455 [==============================] - 0s 76us/step - loss: 65.0306\n",
      "Epoch 6/100\n",
      "455/455 [==============================] - 0s 60us/step - loss: 62.7755\n",
      "Epoch 7/100\n",
      "455/455 [==============================] - 0s 53us/step - loss: 70.2089\n",
      "Epoch 8/100\n",
      "455/455 [==============================] - 0s 59us/step - loss: 63.9513\n",
      "Epoch 9/100\n",
      "455/455 [==============================] - 0s 45us/step - loss: 68.8848\n",
      "Epoch 10/100\n",
      "455/455 [==============================] - 0s 61us/step - loss: 67.4225\n",
      "Epoch 11/100\n",
      "455/455 [==============================] - 0s 51us/step - loss: 68.5933\n",
      "Epoch 12/100\n",
      "455/455 [==============================] - 0s 59us/step - loss: 64.0254\n",
      "Epoch 13/100\n",
      "455/455 [==============================] - 0s 54us/step - loss: 64.8034\n",
      "Epoch 14/100\n",
      "455/455 [==============================] - 0s 44us/step - loss: 63.0519\n",
      "Epoch 15/100\n",
      "455/455 [==============================] - 0s 50us/step - loss: 65.1302\n",
      "Epoch 16/100\n",
      "455/455 [==============================] - 0s 57us/step - loss: 62.3051\n",
      "Epoch 17/100\n",
      "455/455 [==============================] - 0s 48us/step - loss: 64.3658\n",
      "Epoch 18/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 63.7601\n",
      "Epoch 19/100\n",
      "455/455 [==============================] - 0s 61us/step - loss: 63.4209\n",
      "Epoch 20/100\n",
      "455/455 [==============================] - 0s 42us/step - loss: 65.8336\n",
      "Epoch 21/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 63.6115\n",
      "Epoch 22/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 61.6258\n",
      "Epoch 23/100\n",
      "455/455 [==============================] - 0s 61us/step - loss: 63.9940\n",
      "Epoch 24/100\n",
      "455/455 [==============================] - 0s 45us/step - loss: 61.3390\n",
      "Epoch 25/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 60.8903\n",
      "Epoch 26/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 61.6666\n",
      "Epoch 27/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 63.2310\n",
      "Epoch 28/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 63.4079\n",
      "Epoch 29/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 59.3509\n",
      "Epoch 30/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 62.8064\n",
      "Epoch 31/100\n",
      "455/455 [==============================] - 0s 67us/step - loss: 58.6535\n",
      "Epoch 32/100\n",
      "455/455 [==============================] - 0s 66us/step - loss: 64.5363\n",
      "Epoch 33/100\n",
      "455/455 [==============================] - 0s 46us/step - loss: 63.1522\n",
      "Epoch 34/100\n",
      "455/455 [==============================] - 0s 40us/step - loss: 61.9097\n",
      "Epoch 35/100\n",
      "455/455 [==============================] - 0s 45us/step - loss: 61.7661\n",
      "Epoch 36/100\n",
      "455/455 [==============================] - 0s 53us/step - loss: 62.9834\n",
      "Epoch 37/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 59.2960\n",
      "Epoch 38/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 59.1638\n",
      "Epoch 39/100\n",
      "455/455 [==============================] - 0s 40us/step - loss: 60.5857\n",
      "Epoch 40/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 60.4867\n",
      "Epoch 41/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 59.3721\n",
      "Epoch 42/100\n",
      "455/455 [==============================] - 0s 51us/step - loss: 58.6596\n",
      "Epoch 43/100\n",
      "455/455 [==============================] - 0s 43us/step - loss: 59.3357\n",
      "Epoch 44/100\n",
      "455/455 [==============================] - 0s 45us/step - loss: 59.4735\n",
      "Epoch 45/100\n",
      "455/455 [==============================] - 0s 41us/step - loss: 59.6478\n",
      "Epoch 46/100\n",
      "455/455 [==============================] - 0s 40us/step - loss: 60.7757\n",
      "Epoch 47/100\n",
      "455/455 [==============================] - 0s 40us/step - loss: 61.4281\n",
      "Epoch 48/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 57.9942\n",
      "Epoch 49/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 59.5234\n",
      "Epoch 50/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 57.6371\n",
      "Epoch 51/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 64.4314\n",
      "Epoch 52/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 61.5062\n",
      "Epoch 53/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 61.3312\n",
      "Epoch 54/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 59.3729\n",
      "Epoch 55/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 59.4508\n",
      "Epoch 56/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 59.3308\n",
      "Epoch 57/100\n",
      "455/455 [==============================] - 0s 37us/step - loss: 65.8753\n",
      "Epoch 58/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 58.2885\n",
      "Epoch 59/100\n",
      "455/455 [==============================] - 0s 41us/step - loss: 59.0844\n",
      "Epoch 60/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 56.7256\n",
      "Epoch 61/100\n",
      "455/455 [==============================] - 0s 42us/step - loss: 60.0151\n",
      "Epoch 62/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 57.1052\n",
      "Epoch 63/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 62.8178\n",
      "Epoch 64/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 59.4406\n",
      "Epoch 65/100\n",
      "455/455 [==============================] - 0s 43us/step - loss: 57.4205\n",
      "Epoch 66/100\n",
      "455/455 [==============================] - 0s 46us/step - loss: 57.0011\n",
      "Epoch 67/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 57.4005\n",
      "Epoch 68/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 57.6067\n",
      "Epoch 69/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 59.8363\n",
      "Epoch 70/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 62.2535\n",
      "Epoch 71/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 60.6601\n",
      "Epoch 72/100\n",
      "455/455 [==============================] - 0s 37us/step - loss: 61.1178\n",
      "Epoch 73/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 61.4780\n",
      "Epoch 74/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 57.6198\n",
      "Epoch 75/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 61.3348\n",
      "Epoch 76/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 65.5193\n",
      "Epoch 77/100\n",
      "455/455 [==============================] - 0s 37us/step - loss: 63.5027\n",
      "Epoch 78/100\n",
      "455/455 [==============================] - 0s 46us/step - loss: 55.4935\n",
      "Epoch 79/100\n",
      "455/455 [==============================] - 0s 53us/step - loss: 76.2041\n",
      "Epoch 80/100\n",
      "455/455 [==============================] - 0s 77us/step - loss: 57.8326\n",
      "Epoch 81/100\n",
      "455/455 [==============================] - 0s 51us/step - loss: 56.1653\n",
      "Epoch 82/100\n",
      "455/455 [==============================] - 0s 52us/step - loss: 57.8168\n",
      "Epoch 83/100\n",
      "455/455 [==============================] - 0s 56us/step - loss: 58.9305\n",
      "Epoch 84/100\n",
      "455/455 [==============================] - 0s 50us/step - loss: 59.1728\n",
      "Epoch 85/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 56.5961\n",
      "Epoch 86/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 57.6941\n",
      "Epoch 87/100\n",
      "455/455 [==============================] - 0s 37us/step - loss: 56.9656\n",
      "Epoch 88/100\n",
      "455/455 [==============================] - 0s 41us/step - loss: 57.5038\n",
      "Epoch 89/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 56.9875\n",
      "Epoch 90/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 57.0514\n",
      "Epoch 91/100\n",
      "455/455 [==============================] - 0s 41us/step - loss: 61.6826\n",
      "Epoch 92/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 55.2225\n",
      "Epoch 93/100\n",
      "455/455 [==============================] - 0s 37us/step - loss: 56.8070\n",
      "Epoch 94/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 56.9950\n",
      "Epoch 95/100\n",
      "455/455 [==============================] - 0s 34us/step - loss: 57.4492\n",
      "Epoch 96/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 59.2180\n",
      "Epoch 97/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 61.8104\n",
      "Epoch 98/100\n",
      "455/455 [==============================] - 0s 37us/step - loss: 56.7712\n",
      "Epoch 99/100\n",
      "455/455 [==============================] - 0s 33us/step - loss: 57.7967\n",
      "Epoch 100/100\n",
      "455/455 [==============================] - 0s 33us/step - loss: 60.1763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb23522630>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Now that we have trained our model we can evaluate it on our testing set. It is also just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  77.74119044285194\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", model.evaluate(x_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss might seem very high and it is, mostly because there aren't very many training points in the dataset (also no effort was put into finding the best model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate predictions for new data that we don't have labels for. Since we don't have new data, I will just demonstrate the idea with our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.500103]\n",
      " [17.210228]\n",
      " [21.271208]\n",
      " [24.404032]\n",
      " [24.134632]\n",
      " [17.377087]\n",
      " [24.758228]\n",
      " [27.53759 ]\n",
      " [23.742943]\n",
      " [17.780983]\n",
      " [17.487202]\n",
      " [13.676064]\n",
      " [21.007883]\n",
      " [23.259916]\n",
      " [28.138191]\n",
      " [15.968636]\n",
      " [26.957787]\n",
      " [15.515013]\n",
      " [17.024624]\n",
      " [17.747072]\n",
      " [24.671598]\n",
      " [17.822739]\n",
      " [16.146578]\n",
      " [24.954596]\n",
      " [22.785511]\n",
      " [23.23774 ]\n",
      " [23.420858]\n",
      " [27.226002]\n",
      " [16.572668]\n",
      " [22.07244 ]\n",
      " [24.426702]\n",
      " [22.742252]\n",
      " [17.311947]\n",
      " [22.230242]\n",
      " [22.221144]\n",
      " [17.560791]\n",
      " [22.01658 ]\n",
      " [22.901869]\n",
      " [18.178692]\n",
      " [23.108643]\n",
      " [25.052607]\n",
      " [24.658672]\n",
      " [24.818844]\n",
      " [27.391256]\n",
      " [19.714716]\n",
      " [24.098427]\n",
      " [17.589348]\n",
      " [22.009045]\n",
      " [20.709646]\n",
      " [22.132318]\n",
      " [16.907759]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(x_test)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We have successfully (depending on your definition of success) built a fully connected neural network and trained that network on a dataset. Now it's your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 1: Image Classification\n",
    "We are going to build a convolutional neural network to predict image classes on CIFAR-10, a dataset of images of 10 different things (i.e. 10 classes). Things like airplanes, cars, deer, horses, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Load the cifar10 dataset from Keras. If you need a hint go to [Keras Datasets](https://keras.io/datasets). This might take a little while to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(cifar_x_train, cifar_y_train), (cifar_x_test, cifar_y_test) = (x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Initialize a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Add a ``Conv2D`` layer to the model. It should have 32 filters, a 5x5 kernel, and a 1x1 stride. The documentation [here](https://keras.io/layers/convolutional/#conv2d) will be your friend for this problem. __Hint:__ This is the first layer of the model so you have to specify the input shape. I recommend printing ``cifar_x_train.shape``, to get an idea of what the shape of the data looks like. Then add a ```relu``` activation layer to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from keras.layers.convolutional import Conv2D\n",
    "print(cifar_x_train.shape)\n",
    "##YOUR CODE HERE\n",
    "cifar_model.add(Conv2D(32, (5, 5), strides =(1,1), input_shape = (32, 32, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Add a ``MaxPooling2D`` layer to the model. The layer should have a 2x2 pool size. The documentation for Max Pooling is [here](https://keras.io/layers/pooling/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.pooling import MaxPooling2D\n",
    "#Max pooling is meant to reduce the size \n",
    "\n",
    "cifar_model.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Add another ``Conv2D`` identical to last one, then another ``relu`` activation, then another ``MaxPooling2D`` layer. __Hint:__ You've already written this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##YOUR CODE HERE\n",
    "cifar_model.add(Conv2D(32, (5, 5), input_shape = (32, 32, 3), activation = 'relu'))\n",
    "cifar_model.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Add another ``Conv2D`` layer identical to the others except with 64 filters instead of 32. Add another ``relu`` activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##YOUR CODE HERE\n",
    "cifar_model.add(Conv2D(64, (5, 5), input_shape = (32, 32, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)** Now we want to move from 2D data to 1D vectors for classification, to this we have to flatten the data. Keras has a layer for this called [Flatten](https://keras.io/layers/core/#flatten). Then add a ``Dense`` (fully connected) layer with 64 neurons, a ``relu`` activation layer, another ``Dense`` layer with 10 neurons, and a ``softmax`` activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "cifar_model.add(Flatten())\n",
    "cifar_model.add(Dense(units = 64, activation = 'relu'))\n",
    "cifar_model.add(Dense(10, activation='softmax'))\n",
    "##YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have constructed a network that takes in an image and outputs a vector of 10 numbers and then we take the softmax of these, which leaves us with a vector of 0s except 1 one and the location of this one in the vector corresponds to which class the network is predicting for that image. This is sort of the canonical way of doing image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** Now print a summary of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 32)        2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 10, 10, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 1, 1, 64)          51264     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 84,138\n",
      "Trainable params: 84,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##YOUR CODE HERE\n",
    "#what does this summary mean exactly?\n",
    "cifar_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i)** We need to convert our labels from integers to length 10 vectors with 9 zeros and 1 one, where the integer label is the index of the 1 in the vector. Luckily, Keras has a handy function to do this for us. Have a look [here](https://keras.io/utils/#to_categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_cat = ???\n",
    "y_test_cat = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(j)** Now compile the model with SGD optimizer and categorical_crossentropy loss function and also include ``metrics=['accuracy']`` as a parameter so we can see the accuracy of the model. Then train the model on the training data. For training we want to weight the classes in the loss function, so set the ``class_weight`` parameter of fit to be the ``class_weights`` dictionary. Be warned training can take forever, I trained on a cpu for 20 epochs (about 30 minutes) and only got 20% accuracy. For the purposes of this assignment, you don't need to worry to much about accuracy, just train for at least 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##YOUR COMPILING CODE HERE\n",
    "cifar_model.compile(optimizer = 'SGD', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "for i in range(10):\n",
    "    class_weights[i] = 1. / np.where(cifar_y_train==i)[0].size\n",
    "\n",
    "##YOUR TRAINING CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar_model.evaluate(cifar_x_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the class labels the network predicts on our test set and look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = cifar_model.predict(cifar_x_test)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(cifar_x_test[1234])\n",
    "print(\"Predicted label: \", np.argmax(y_pred[1234]))\n",
    "print(\"True label: \", cifar_y_test[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will use Kera's imdb sentiment dataset. You will take in sequences of words and use an RNN to try to classify the sequences sentiment. First we have to process the data a little bit, so that we have fixed length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    processed = np.zeros(len(data) * 200).reshape((len(data), 200))\n",
    "    for i, seq in enumerate(data):\n",
    "        if len(seq) < 200:\n",
    "            processed[i] = np.array(seq + [0 for _ in range(200 - len(seq))])\n",
    "        else:\n",
    "            processed[i] = np.array(seq)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3913, 200)\n",
      "(25000, 200)\n"
     ]
    }
   ],
   "source": [
    "x_train_proc = process_data(x_train)\n",
    "x_test_proc = process_data(x_test)\n",
    "print(x_test_proc.shape)\n",
    "print(x_train_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding Layer is a little bit different from most of the layers, so we have provided that code for you below. Basically, the 1000 means that we are using a vocabulary size of 1000, the 32 means we will have a vector of size 32 as the output, and the `mask_zero` means that we don't care about 0 because we are using it for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "imdb_model.add(Embedding(1000, 32, input_length=200, mask_zero=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** For this problem, I won't walk you everything like I did in the last one. What you need to do is as follows. Add an LSTM layer with 32 outputs, then a Dense layer with 32 neurons, then a relu activation, then a dense layer with 1 neuron, then a sigmoid activation. Then you should print out the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 32)                29824     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,913\n",
      "Trainable params: 30,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##YOUR CODE HERE\n",
    "from keras.layers import LSTM\n",
    "imdb_model.add(LSTM(32, input_shape=(25000, 200)))\n",
    "imdb_model.add(Dense(units = 32, activation = 'relu'))\n",
    "imdb_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "imdb_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Now compile the model with binary cross entropy, and the Adam optimizer. Also include accuracy as a metric in the compile. Then train the model on the processed data (no need to worry about class weights this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##YOUR CODE HERE\n",
    "#Incomplete... did not train the data\n",
    "cifar_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we can evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-b4fb3727d6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_proc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                 raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m    682\u001b[0m                                    \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                                    'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", imdb_model.evaluate(x_test_proc, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at our predictions and the sentences they correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_5_input to have 3 dimensions, but got array with shape (3913, 200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-7f37c0f48081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_5_input to have 3 dimensions, but got array with shape (3913, 200)"
     ]
    }
   ],
   "source": [
    "y_pred = imdb_model.predict(x_test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 2s 1us/step\n",
      "['the', 'and', 'and', \"film's\", 'and', 'in', 'an', 'like', 'he', 'his', 'as', 'it', 'of', 'sure', 'br', 'and', 'entertaining', 'way', 'who', 'of', 'sure', 'felt', 'br', 'and', 'and', 'of', 'their', 'it', 'rest', 'to', 'and', 'of', 'too', 'and', 'to', 'and', 'change', 'and', 'through', 'but', 'that', 'what', 'and', 'pretty', 'in', 'at', 'and', 'in', 'home', 'i', 'i', 'of', 'on', 'and', 'film', 'of', 'and', 'br', 'and', 'this', 'and', 'new', 'and', 'is', 'quite', 'br', 'that', 'it', \"wasn't\", 'which', 'years', 'this', 'and', 'and', 'and', 'not', 'his', 'in', 'of', 'and', 'movie', 'and', 'reason', 'most', 'make', 'is', 'certain', 'on', 'i', 'i', 'more', 'he', 'brings', 'br', 'because', 'and', 'characters', 'viewer', 'who', 'told', 'and', 'disney', 'for', 'he', 'main', 'and', 'as', 'by', 'it', 'is', 'and', 'sequence', 'character', 'is', 'novel', 'john', \"it's\", 'few', 'she', 'and', 'are', 'of', 'and', 'what', 'have', 'he', 'and', 'comment', 'this', 'of', 'and', 'br', 'of', 'and', 'this', 'and', 'i', 'i', 'was', 'and', 'only', 'my']\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.vectorize(lambda x: int(x >= 0.5))(y_pred)\n",
    "correct = []\n",
    "incorrect = []\n",
    "for i, pred in enumerate(y_pred):\n",
    "    if y_test[i] == pred:\n",
    "        correct.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)\n",
    "word_dict = inv_map = {v: k for k, v in imdb.get_word_index().items()}\n",
    "\n",
    "print(list(map(lambda x: word_dict[int(x)] if x != 0 else None, x_test[correct[123]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making this I realized that `keras`'s method for converting from word index back to words is broken right now (see this open [github issue](https://github.com/fchollet/keras/issues/5912)). So we can't actually see what the sentences look like."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
